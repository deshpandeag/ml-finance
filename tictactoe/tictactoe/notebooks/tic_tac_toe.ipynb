{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The associated codes to run this notebook can be found [here](https://github.com/prodipta/tictactoe). Clone the repo, and open this notebook (notebooks/tic_tac_toe.ipynb).\n",
    "\n",
    "## Reinforcement Learning: An Introduction\n",
    "\n",
    "Reinforcement learning is a machine learning algorithm for general purpose decision making. Unlike traditional supervised learning, we do not have labelled data to train on. Instead we rely on trying out some actions and observe the reward associated with the actions to learn.\n",
    "\n",
    "### Reinforcement Learning Applications\n",
    "\n",
    "The class of problems where an agent need to interact with an environment to optimize multi-period decision makings are tailor-made for reinforcement learning. The set of problems in this class are vast, from controlling a robots to playing games to figuring out optimal portfolio building. Any problem that we can cast as [Markov decision process](https://en.wikipedia.org/wiki/Markov_decision_process) are candidate for reinforcement learnings.\n",
    "\n",
    "- Is the decision making is multi-period? Categorizing a picture as `cat` or `not cat` is not a multi-period. We can classify immediately based on features of the image and get a reward (correct classification) or penalty (wrong classification). However, if a particular move in a game of chass is good or not - this is a multi-period decision making problem. The rewards are not immediately apparent after we make the decision, but only available at the end of the game.\n",
    "\n",
    "Markov decision process has been investigated since 1950s. Reinforcement learning is a relatively new concepts that borrows heavily from earlier developments (dynamic programming) and uses new techniques (machine learning) to solve such problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment, Agent, State and Action\n",
    "\n",
    "Reinforcement learning is typically set up by modelling the interaction between two entities - the `agent` and the `environment`. The agent interacts with the environment and tries to learn (i.e. optimize its decision making). Examples are\n",
    "\n",
    "- In the game of chess - the agent is the player, and the environment is everything else, including the board and the opponents\n",
    "\n",
    "- A robot trying to walk across a terrain - the agent is the robot, and the environment is the terrain including its geometry and the physics\n",
    "\n",
    "The interaction between the agent and the environment can be captured as below:\n",
    "\n",
    "<img src=\"resources/agent-env.png\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "To start the learning process, the agent commits an `action`. This action can be based on previous learning, or random if none available. This modifies the state of the environment - it goes from current state to the next state. It also generate a reward associated with the state change. The agent gets these new information - the reward and the state change information. In the next iteration, it draws on these observation and try to optimize its action towards a goal.\n",
    "\n",
    "Note, reward associated with a particular action can be zero. Usually the bulk of the rewards are realized at the end, when the game ends (when the game of chess is won or lost, or when the robot successfully walks across the field, or fails). If all steps have equivalent rewards then the problem is no longer a multi-period decision. That is, our action at present has no impacts on the outcome in distant future. In this case, the problem degenates to much like identifying a cat picture - the realm or regular (supervised) machine learning.\n",
    "\n",
    "## General Problem Statement\n",
    "\n",
    "You are interacting with an `environment`. At each `state` of the environment, you must choose an allowed `action`. After each such `action`, you may or may not receive an immediate `reward` (or penalty, a negative reward). After an uncertain n `steps` (or `state` changes), the `environment` reaches its final `state` and you realize the final `reward` (or penalty).\n",
    "\n",
    "- In financial investing, the `environment` is the market itself. Your `action` is either buy, sell or hold of the stocks in the universe, and rewards are profit and loss.\n",
    "\n",
    "- In a game of poker, the `environment` is your opponents collectively, (only one in heads-up poker, a mathematically easier version). Your `actions` are fold, bet, call, raise or re-raise. Your rewards are realized only after each hands (or at the end of the whole game if it is a tournament)\n",
    "\n",
    "- In a game of chess, the `environment` is the board and the opponent. The `state` is the current configuration of the board. Your `actions` are defined by the chess rules and your remaining chess pieces. The `reward` is received only when the game is over.\n",
    "\n",
    "All these decision making problems are similar in nature, although they vary greatly with each parameters - `state`, `actions` and `rewards`. \n",
    "\n",
    "For example, in tic-tac-toe, the `state` are finite and relatively small. In chess, `states` are much more numerous, but they are still somewhat manageable. In poker, `states` are never known fully and can only be guessed - you do not get to see your opponents hand. In markets `state` is infinite - an endless combinations of varius market factors, prices and sentiments etc.\n",
    "\n",
    "We aim to find a generalized approach to solve such problems. We try to crack the tic-tac-toe game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Concepts - Exploration, Discounting and Rewards\n",
    "\n",
    "Since we are trying to solve a multi-period decision making problem, we need to be careful about how we approach this. One way to optimize decisions in such a setting is to try and optimize each individual decision - and hoping the whole optimization is sum of these individual optimization problems. As mentioned earlier, for certain types of rewards, that can be the case, but not in general. Therefore although it is tempting to optimize the current step, we must not succumb to it blindly. That is, we keep a balance between `exploitation` (optimizing the current decision) and `exploration` (randomizing the current decision to allows us to find the gloabl optimum). This balance of `exploitation` vs `exploration` is an important concept in this class of problems. The parameter that captures this balance is usually termed `epsilon (ε)` in the literature of reinforcement learning.\n",
    "\n",
    "The second issue in multi-period problem is how we add up rewards received over a period of steps - over a time. Remember, we try to optimize the total reward, not reward at each step. In general most of us prefer `immediacy` vs `deferred` rewards. So we usually use a factor to discount rewards received in the future - much like how we calculate [net present value](https://en.wikipedia.org/wiki/Net_present_value) - discouting future receivables by a factor. The choice of this discount factor can impact learning. In the literature, it is usually referred to as `gamma (γ)`.\n",
    "\n",
    "Finally, the training process is all about rewards - as this is the single metric that it tries to optimize. So we need to be very careful. This usually requires a good knowledge of the field of application. Designing the reward function carefully to improve learnings is also known as `reward engineering`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tic-Tac-Toe: A demo for reinforcement learning\n",
    "\n",
    "from [wikipedia](https://en.wikipedia.org/wiki/Tic-tac-toe): Tic-tac-toe (American English), noughts and crosses (Commonwealth English), or Xs and Os is a paper-and-pencil game for two players, X and O, who take turns marking the spaces in a 3×3 grid. The player who succeeds in placing three of their marks in a horizontal, vertical, or diagonal row is the winner.\n",
    "\n",
    "### The decision making problem\n",
    "\n",
    "Suppose we start with the 'X'. The decision problem is choose a series of action that leads to a potential win. The `reward` - win, loss, or a draw, comes at the end of the game. To get there, we need to make a series of decisions where we do not get immedeate clue if our decision was correct or wrong. What are the way(s) to create an algorithm to win?\n",
    "\n",
    "### A common man's approach\n",
    "\n",
    "Start with a random choice, say top corner. Then, each time it is our turn to play, do the following in order:\n",
    "\n",
    "- block an impending triad completion of our opponent, if any\n",
    "- Else, we put a 'X' to that makes `possible for us to complete a triad in the next move` in case our opponent does not notice it.\n",
    "\n",
    "This is probably be a decent strategy for a beginner.\n",
    "\n",
    "### A mathametical approach\n",
    "\n",
    "Proposed in 1972, by [Newell and Simon](https://en.wikipedia.org/wiki/Tic-tac-toe#Strategy). It is a collection of heuristics that relies on combinatorics. The [game complexity](https://en.wikipedia.org/wiki/Game_complexity#:~:text=For%20tic%2Dtac%2Dtoe%2C,have%20a%20row%20of%20three.) is relatively mild - total 19,683 possible configuration - reduces drastically if we remove invalid configurations (e.g.  five crosses and no noughts).\n",
    "\n",
    "### What about variations\n",
    "\n",
    "This strategies quickly become more complicated as we expand the game - say `nxn` board with first `k` to finish is a winner. It can also be complicated, e.g. instead of only two characters what if we have 8, each with different rules (chess!). Or say the two players are playing on private boards not visible to each other (not a very good example in this case, these types of games are called `incomplete information`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Environment\n",
    "\n",
    "The first thing to do to apply reinforcement learning to solve this problem is to create an appropriate environment for tic-tac-toe, that captures the game rules and opponents behaviour. Remember, at a basic level, an environment keep track of its states, accepts an action at any given state and returns the information about the associated reward and state change the action may have caused.\n",
    "\n",
    "There are many ways to model `environment` for reinforcement learning. We follow the standard [gym](https://gym.openai.com/) interface from the [OpenAPI](https://www.openapis.org/). In this set-up, we model the tic-tac-toe environment as a Python [class](https://docs.python.org/3/tutorial/classes.html) that implements three main methods:\n",
    "\n",
    "- `step` method takes in an `action` and generates the associated `reward`, and next `state`. It also has a `done` parameter that signifies if the game is over - i.e. mark the reward as terminal reward.\n",
    "\n",
    "- `reset` method reset the environment, ready to undergo another round of game in case of tic-tac-toe\n",
    "\n",
    "- `render` method optionally allows us to visualize the board if we wish so.\n",
    "\n",
    "#### How to Run the Codes\n",
    "\n",
    "The implementation of this class (and others) is the in this Github repo. Please [clone](https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository) or copy the repo locally. Then go inside the outer `tictactoe` folder and run setup. If you already have `gym`, `numpy`, `pandas` and `tqdm` installed, you can skip this step.\n",
    "\n",
    "```sh\n",
    "cd tictactoe\n",
    "python setup.py sdist bdist_wheel\n",
    "cd ..\n",
    "pip install -e tictactoe\n",
    "```\n",
    "You can also install the packages separately, in that case you can skip the commands above. To run this Notebook, open the notebook from the inner `tictactoe` under the `notebooks` folder and directly run the cells below. \n",
    "\n",
    "Now let's simulate some games. In this particular implementation, the first mover is modelled as player `1` - that is us (or the `agent`) playing. The opponent is player `2` (part of the environment). A draw is marked by `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import_path = os.path.abspath(os.path.join('..'))\n",
    "if import_path not in sys.path:\n",
    "    sys.path.append(import_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0  |  1  |  2  \n",
      "-----+-----+-----\n",
      "  2  |  1  |  1  \n",
      "-----+-----+-----\n",
      "  2  |  1  |  2  \n",
      "\n",
      "the winner is 1\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from tictactoe.envs import TictactoeEnv\n",
    "\n",
    "env = TictactoeEnv()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()    # choose a random action from available moves\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "env.render()\n",
    "print(f'the winner is {env.winner}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick benchmark\n",
    "\n",
    "Before we start creating the data and model for our ML tic-tac-toe player, let's establish a benchmark to evaluate it against. If we play randomly (and the board plays against us randomly as well), we should have approx 1/3 changes of win, losses and draws. Since we are the first-mover, we will have slight advantage in a game like tic-tac-toe (and chess and similar full-information games). Let's simulate a games and score it when both players play randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "played 500 games, wins%:32.2, losses%:29.8, draws%:38.0\n"
     ]
    }
   ],
   "source": [
    "N = 500\n",
    "wins = losses = draws = 0\n",
    "\n",
    "for _ in range(N):\n",
    "    env.reset() # reset the board\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "    if env.winner == 1:\n",
    "        wins += 1\n",
    "    elif env.winner == 2:\n",
    "        losses += 1\n",
    "    else:\n",
    "        draws += 1\n",
    "\n",
    "wins = round(100*(wins/N), 2)\n",
    "losses = round(100*(losses/N), 2)\n",
    "draws = round(100*(draws/N), 2)\n",
    "print(f'played {N} games, wins%:{wins}, losses%:{losses}, draws%:{draws}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the random bench mark gives us a slight advantange (33% win against 32% loss, figure above may vary!). Let's see if we can do better with our ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a decision making framework\n",
    "\n",
    "We take the reinforcement learning approach to solve this problem. Recall our previous discussions: \n",
    "\n",
    "- Initially we play randomly and observe the results, and try to learn from it\n",
    "- we aim to make total rewards best for us (optimize total rewards, than rewards at each step)\n",
    "- we follow a strategy to balance exploitation vs. exploration\n",
    "- we use discounting to normalize delayed rewards \n",
    "\n",
    "What should be our objective for decision making\n",
    "\n",
    "### Maximize the value of our decision\n",
    "\n",
    "One good approach is to choose a series of `actions` that try and maximize the total expected `reward` (expected immediate and future rewards based on current information, or mathematically, conditioned on current `state`)\n",
    "\n",
    "\\begin{align}\n",
    "V_{0}^\\pi(s) = \\mathbb E[R_{0}(s_{0},a_{0}) + \\gamma.R_{1}(s_{1},a_{1}) + \\gamma^2.R_{2}(s_{2},a_{2}) ....|s_{0}=s,\\pi]\n",
    "\\end{align}\n",
    "\n",
    "Here `R` is the reward, `s` is the environment state and `a` is the action performed by the `agent` at a given time (subscripts numbers). Note, how we are using the `𝛾` to discount future rewards. Or in other words\n",
    "\n",
    "\\begin{align}\n",
    "V_{0}^\\pi(s) = \\mathbb E\\Bigl[\\sum_n(\\gamma^n.R_{n}(s_{n},a_{n}))|s_{0}=s,\\pi\\Bigr]\n",
    "\\end{align}\n",
    "\n",
    "We aim to find `policy (𝜋)` such that we maximize this expression. One issue with the above expression is that it tells us what is the expected worth of our actions, but does not tell us what to do. So let's recast the above such that the `action` is a parameter as well, not just `state`.\n",
    "\n",
    "\\begin{align}\n",
    "V_{0}^\\pi(s,a) = \\mathbb E\\Bigl[\\sum_n(\\gamma^n.R_{n}(s_{n},a_{n}))|s_{0}=s, a_{0}=a,\\pi\\Bigr]\n",
    "\\end{align}\n",
    "\n",
    "This makes the first term deterministic, so we take this out and the expression becomes\n",
    "\n",
    "\\begin{align}\n",
    "Q_{0}^\\pi(s,a) = R_{0}(s,a) + \\mathbb E\\Bigl[\\sum_{n=1}(\\gamma^n.R_{n}(s_{n},a_{n}))|s_{0}=s, a_{0}=a,\\pi\\Bigr]\n",
    "\\end{align}\n",
    "\n",
    "### The Bellman Equation\n",
    "\n",
    "Next, we do two simple changes - first, we change the subscript of the value Q to variable time, to make the equation dynamic explicitly. Then, we notice that the second part of the above equation is just gamma times the value Q itself. So we recast a dynamic version of the above equation as follows:\n",
    "\n",
    "\\begin{align}\n",
    "Q_{t}^\\pi(s,a) = R_{t}(s,a) + \\gamma.Q_{t+1}^\\pi(s,a)\n",
    "\\end{align}\n",
    "\n",
    "From the above, and from Bellman's [principle of optimality](https://en.wikipedia.org/wiki/Bellman_equation), we claim that to maximize the value we must choose Q such that:\n",
    "\n",
    "\\begin{align}\n",
    "Q_{t}^\\pi(s,a) = R_{t}(s,a) + \\gamma.Max(Q_{t+1}^\\pi(s,a))\n",
    "\\end{align}\n",
    "\n",
    "### Q-learning\n",
    "\n",
    "From the above, we get an interesting insight. An algorithm can be developed on the following line:\n",
    "\n",
    "- We create a table (called `Q-table`) that will map our evaluations of each allowed `actions` at each `state`. We start with all zero values.\n",
    "\n",
    "- Next, at initial state, to chose our action `a`, we randomly try all allowed actions and chose the one that provides maximum immediate benifit in the next step.\n",
    "\n",
    "- We remember this favourable action by making an entry in the Q-table, and take the chosen action to move to the next step\n",
    "\n",
    "- Repeat\n",
    "\n",
    "If we do it large enough number of times, we will have enough samples for each `(s,a)` combinations that will allow us average the value computation over and get an expected value. And if these values converge to a limit, we are done. We have created a Q-table, that has seen too many games and memorized all moves to make best judgement best on its memory.\n",
    "\n",
    "\n",
    "### The problem - being myopic \n",
    "\n",
    "Notice in the above example we consider only the immediate rewards. Since all actions in a game are connected, that means we may have chosen to leave behind some paths that had low immediate rewards but high final rewards. We were too exploitive and forgot to explore. This risks settling down on a local minima instead of a global one. \n",
    "\n",
    "One way to avoid this is to introduce a new parameter `epsilon`. It is a fraction between 0 to 1. While choosing the next action, now we will not always choose the best on immediate rewards. With a probability of `epsilon`, we will now choose a random action. And only with probability `(1-epsilon)` we will choose the best. This gives us enough randomness in search to explore good paths that reward later. Also, we can have a high value of `epsilon` at the beginning of learning (`explorative`), and reduce (more `expoliting`) towards the end. A popular way to do this is to introduce an exponential decay of `epslion` between each batch of training.\n",
    "\n",
    "Finally, we can also have a parameter `nu` or `learning-rate`, that we use to balance between the old and new values of updates of the Q-table. Instead of overwriting the past learned value each time, we can use a fraction `nu` of the new value and `(1-nu)` of the old value for the final update. This smoothens out our learning process.\n",
    "\n",
    "How to apply all these to our tic-tac-toe problem.\n",
    "\n",
    "\n",
    "### Learning by Playing - the Agent\n",
    "\n",
    "We already modelled the game. Now we model an `agent` or a player. We store the Q-table and let the `agent` interact with the environment to learn. In each leaning episode, the `agent` play a complete game, choosing actions based on the algorithm above and updating Q-table at each step. In the next episode, it starts with the existing Q-table and carries out further updates and learnings. Under certain [cicumstances](https://en.wikipedia.org/wiki/Markov_decision_process) this gurantess a convergence. \n",
    "\n",
    "There are many ways we can model the agent. But it must support these methods:\n",
    "\n",
    "- `act`: A method that accpets the environment and its current state, and provides the next action to be taken. During the training, initially these will be randomly choosen, and then it should get better and better\n",
    "\n",
    "- `fit`: A method, that implements the training of the agent - essentially for a Q-learner, we follow the algorithm specified above by playing the game repeatedly.\n",
    "\n",
    "- `test`: A method, that tests how good the agent is. It is similar to `fit`, except, it does not train anymore and chooses the best action from its already learned Q-table.\n",
    "\n",
    "The Q-table itself is modelled as a pandas dataframe. The columns are the actions, and the row index captures the states. Initially, there are now rows in the table. As we play the game and see a new state, we add a row corresponding to that with initial 0 values for all actions. Later, if we revisit the state again, we update the Q value according to our algorithm above.\n",
    "\n",
    "We have already implemented an agent in the file `qlearning.py`. There are also some helpers functions there to complement the three functions above that handles updating of the Q table and fetching value from the Q table - these are `get_q`, `get_max_q` and `update`. The training function is `fit`. This loops through a chosen number of iterations (also known as `episodes`) and in each iteration it plays a complete game. During the game, it chooses an action based randomly (based on `epsilon` or if it is a new state never seen before) or best on reward maximization. Then it computes the updates for the corresponding Q value based on the Bellman's equation and update the Q table.\n",
    "\n",
    "Let's try the training now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10000/10000 [02:47<00:00, 59.88it/s]\n"
     ]
    }
   ],
   "source": [
    "from tictactoe.models.qlearning import fit, test\n",
    "\n",
    "env.reset()\n",
    "Q = fit(env, episodes=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:05<00:00, 96.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "played 500, wins%:88.4, losses%:6.4, draws%:5.2\n",
      "  1  |  1  |  2  \n",
      "-----+-----+-----\n",
      "  1  |  2  |  2  \n",
      "-----+-----+-----\n",
      "  1  |  2  |  0  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test(env, Q, episodes=500)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a whooping improvement than the last. Our win rate is close to 90% compared to 56% based on the brute-force machine learning!\n",
    "_______________________________________________________________________________________________________________________________\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
